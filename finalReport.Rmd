---
title: "Modeling Data in the Tidyverse Course Project"
author: "Oswaldo Navarrete"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading packages

```{r, message=F, warning=F}
library(tidyverse)
library(tidymodels)
library(tm)
library(wordcloud)
library(doParallel)
library(vip)
library(skimr)
library(jtools)
```

## Use multiple cores in order to improve run time

```{r}
detectCores()
doParallel::registerDoParallel(cores=8)
```

## Loading data and checking the data structure

The training and the testing dataset have 6 variables, the most important variables in the training set are Product and Consumer.complaint.narrative. In the testing set the only variable to be used is Consumer.complaint.narrative. In the two datasets, the Consumer.complaint.narrative doesn't have NAs. This variable was renamed in both datasets as Complaint, also in the training data the variable product was converted to factor. 

```{r}
training <- read.csv("data_complaints_train.csv")
testing <- read.csv("data_complaints_test.csv")

names(training)
names(testing)

skim(training)
skim(testing)
```

```{r}
training <- training |>
  rename(Complaint = Consumer.complaint.narrative) |>
  select(Product, Complaint) |> 
  mutate(Product = factor(Product))

testing <- testing |>
  rename(Complaint = Consumer.complaint.narrative) |>
  select(Complaint)
```

## Cleaning and preparing the training and testing data sets.

For the two datasets, it's important prepare the Consumer.complaint.narrative because this variable is a long narrative. 

### Training

The first step involved the following processes:

1. The private information has been masked as "XX", "XXX" and "XXXX", these character strings were removed. 
2. All the letters were converted to lower case.
3. The numbers were removed.
4. Punctuation was removed.
5. Special characters and white spaces are removed. 
6. The stop words were removed

```{r}
training_clean <- training |> 
  mutate(Complaint = gsub("[XX]+", "", Complaint)) |> 
  mutate(Complaint = str_to_lower(Complaint)) |> 
  mutate(Complaint = gsub("[0-9]", "", Complaint)) |> 
  mutate(Complaint = removePunctuation(Complaint)) |> 
  mutate(Complaint = gsub("\n", "", Complaint)) |> 
  mutate(Complaint = gsub("\t", "", Complaint)) |> 
  mutate(Complaint = stripWhitespace(Complaint))

training_complaints <- Corpus(VectorSource(training_clean$Complaint)) |>
  tm_map(removeWords, stopwords())
```

A document term matrix was created, each complaint is a document in a row and each column is a term of the document. The value of each cell belongs to the number of times the term appears in the document. 

```{r}
training_docterm <- DocumentTermMatrix(training_complaints)
inspect(training_docterm)
```
The dataset had 81664 terms, so it's convenient to reduce the number of terms. I decided to choose the terms that occur at least 1500 times, this threshold is arbitrary but helps to reduce the number of terms. 

```{r}
limitfreq <- findFreqTerms(training_docterm, lowfreq=1500)
training_docterm <- DocumentTermMatrix(training_complaints,
                                       list(dictionary=limitfreq)) 
inspect(training_docterm)
```
Now, the number of terms decreased to 1155. The sparsity fell down to 95%. The following step is remove the terms with a 95% minimum of sparsity.   

```{r}
training_docterm <- removeSparseTerms(training_docterm, 0.95) 
inspect(training_docterm)
```

The sparsity has reduced to 89% with 341 terms, before starting is a good idea explore the terms in order to determine if exist irrelevant or unnecessary terms.    

```{r}
training_docterm_df <- training_docterm |>
  as.matrix() |>
  as.data.frame()
```

Looking at the terms it's possible realize that there are unnecessary terms so the following step is rebuild the docterm matrix removing previously the "unnecessary" terms and adding the other filters applied before.

```{r}
training_docterm_df  |>
  summarise_all(sum)

list_terms <- sort(names(training_docterm_df))

unnecessary_terms <- c("able","ago","already","also","another",
                       "anything","asked","back","believe",
                       "call","called","calling","can","didnt",
                       "done","dont","either","even","get",
                       "going","got","however","just",
                       "later","like","needed","never","now",
                       "please","put","said","said","saying",
                       "take","today","told","took","used",
                       "want","way","went","will","without")
```

```{r}
training_complaints<-tm_map(training_complaints, removeWords, unnecessary_terms)

training_docterm <- DocumentTermMatrix(training_complaints,
                                       list(dictionary=limitfreq))

training_docterm <- removeSparseTerms(training_docterm, 0.95)
inspect(training_docterm)

training_docterm_df <- training_docterm |>
  as.matrix() |>
  as.data.frame()

dim(training_docterm_df)
```

Finally, the training dataset is combined with the product's information of the training_clean dataset made at the beginning.  

```{r}
training_docterm_df <- training_docterm_df |>
  bind_cols(Product=training_clean$Product) |>
  select(Product, everything())
```


### Testing

For the testing set, only the steps that involve the cleaning, and get the docterm matrix with the frequent terms were made. 

```{r}
testing_clean <- testing |> 
  mutate(Complaint = gsub("[XX]+", "", Complaint)) |> 
  mutate(Complaint = str_to_lower(Complaint)) |> 
  mutate(Complaint = gsub("[0-9]", "", Complaint)) |> 
  mutate(Complaint = removePunctuation(Complaint)) |>
  mutate(Complaint = gsub("\n", "", Complaint)) |> 
  mutate(Complaint = gsub("\t", "", Complaint)) |> 
  mutate(Complaint = stripWhitespace(Complaint))

testing_complaints <- Corpus(VectorSource(testing_clean$Complaint)) |>
  tm_map(removeWords, stopwords())

testing_docterm <- DocumentTermMatrix(testing_complaints, list(dictionary=limitfreq)) 

testing_df <- testing_docterm |>
  as.matrix() |>
  as.data.frame()
```

## Building a ML model for predicting the product's category

### Train and validation sets

The training_docterm_df was splitted to train (75%) and validate (25%) the model. 

```{r}
set.seed(1110)
training_df_split <- rsample::initial_split(data = training_docterm_df, prop = 3/4)
train_df <- training(training_df_split)
valid_df <- testing(training_df_split)
```

### Cross validation folds 

A 10-fold cross validation was created. 

```{r}
vfold_df <- rsample::vfold_cv(data = train_df, v = 10)
vfold_df
```

### Machine learning workflow

#### Recipe

```{r}
train_df_rec <- train_df |>
  recipe(Product ~ .) |>
  step_corr(all_predictors()) |>
  step_nzv(all_predictors())
```

#### Model

```{r}
train_df_model <- rand_forest(mtry = 10, min_n = 4) |>
  set_engine("randomForest") |>
  set_mode("classification")
```

#### Workflow

```{r}
train_df_wflow <- workflow() |>
  add_recipe(train_df_rec) |>
  add_model(train_df_model)
```

### Fit and model performance

After setting the recipe, model, and workflow the workflow was fitted. With the fitted model, the importance of the variables was analyzed. According to the importance plot, the most important words for the prediction were: mortgage, card and loan. 

```{r}
train_df_fit <- fit(train_df_wflow, data = train_df)

train_df_wflow_fit <- train_df_fit |>
  extract_fit_parsnip()|>
  vip(num_features = 10) +
  theme_apa()

train_df_wflow_fit
```
#### Fitting to resamples 

Using cross validation, the model performance was evaluated.

```{r}
set.seed(1110)

resample_train_df_fit <- tune::fit_resamples(train_df_wflow, 
                                             vfold_df, 
                                             control = control_resamples(save_pred = TRUE))

collect_metrics(resample_train_df_fit)
```

The values of the accuracy and the ROC AUC were $84.39 \%$ and $0.9368$ respectively. 

### Model tuning

Although the metrics are good, a model tuning was probed. 

```{r}
tune_RF_model <- rand_forest(mtry = tune(), min_n = tune()) |>
  set_engine("randomForest") |>
  set_mode("classification")
```

```{r}
RF_tune_wflow <- workflows::workflow() |>
  workflows::add_recipe(train_df_rec) |>
  workflows::add_model(tune_RF_model)
```

```{r}
doParallel::registerDoParallel(cores=8)

set.seed(1110)

tune_RF_results <- tune_grid(object = RF_tune_wflow, resamples = vfold_df, grid = 20 )
```

### Performance

```{r}
show_best(tune_RF_results, metric="accuracy")
```
The accuracy raised up to $85.41\%$, with the best model the workflow was updated. 

```{r}
tuned_values<-select_best(tune_RF_results,"accuracy")
RF_tuned_wflow <- RF_tune_wflow |>
  finalize_workflow(tuned_values)
```

## Final Model Performance Evaluation

With the complete training data set the final model was fitted. The accuracy is $84.99\%$. 

```{r}
overall_fit<-last_fit(RF_tuned_wflow, training_df_split) 
collect_metrics(overall_fit)
```

```{r}
test_predictions<-collect_predictions(overall_fit) 
head(test_predictions)
```

### Prediction's visualization

```{r}
ggplot(test_predictions, aes(x=Product, fill=.pred_class)) + 
     geom_bar(position="fill", color="black") +
     scale_fill_brewer(palette="Set2") + labs(x="Actual Outcome Values",
     y="Proportion", fill="Predicted Outcome") +
     theme_apa() +
     theme(axis.text.x=element_text(angle=45, hjust=1, vjust=1))
```

The model works well for all the categories except for *Vehicle loan or lease*.

## Prediction for the test data

Finally, the prediction is made for the test data provided in the instructions. 

```{r}
final_model<-fit(RF_tuned_wflow, training_docterm_df) 
predict(final_model, new_data=testing_df)
```

